{"format": "torch", "nodes": [{"name": "encoder", "id": 140623998357072, "class_name": "RNNEncoder(\n  (dropout): Dropout(p=0.1, inplace=False)\n  (input_dropout): UnknownDropout()\n  (lt): Embedding(22419, 128, padding_idx=0)\n  (rnn): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.1)\n)", "parameters": [["lt.weight", [22419, 128]], ["rnn.weight_ih_l0", [512, 128]], ["rnn.weight_hh_l0", [512, 128]], ["rnn.bias_ih_l0", [512]], ["rnn.bias_hh_l0", [512]], ["rnn.weight_ih_l1", [512, 128]], ["rnn.weight_hh_l1", [512, 128]], ["rnn.bias_ih_l1", [512]], ["rnn.bias_hh_l1", [512]]], "output_shape": [[16, 64, 128], [[16, 2, 128], [16, 2, 128]], [16, 64]], "num_parameters": [2869632, 65536, 65536, 512, 512, 65536, 65536, 512, 512]}, {"name": "decoder", "id": 140623998346896, "class_name": "RNNDecoder(\n  (dropout): Dropout(p=0.1, inplace=False)\n  (lt): Embedding(22419, 128, padding_idx=0)\n  (rnn): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.1)\n  (attention): AttentionLayer(\n    (attn_combine): Linear(in_features=256, out_features=128, bias=False)\n  )\n)", "parameters": [["lt.weight", [22419, 128]], ["rnn.weight_ih_l0", [512, 128]], ["rnn.weight_hh_l0", [512, 128]], ["rnn.bias_ih_l0", [512]], ["rnn.bias_hh_l0", [512]], ["rnn.weight_ih_l1", [512, 128]], ["rnn.weight_hh_l1", [512, 128]], ["rnn.bias_ih_l1", [512]], ["rnn.bias_hh_l1", [512]], ["attention.attn_combine.weight", [128, 256]]], "output_shape": [[16, 2, 128], [[16, 2, 128], [16, 2, 128]]], "num_parameters": [2869632, 65536, 65536, 512, 512, 65536, 65536, 512, 512, 32768]}, {"name": "output", "id": 140623995304912, "class_name": "OutputLayer(\n  (dropout): Dropout(p=0.1, inplace=False)\n  (o2e): Identity()\n)", "parameters": [["bias", [22419]], ["weight", [22419, 128]]], "output_shape": [[16, 2, 22419]], "num_parameters": [22419, 2869632]}], "edges": []}