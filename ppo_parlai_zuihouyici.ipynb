{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ppo_parlai_zuihouyici.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1MyBrBxEZOZ-D1ioP-aOENE1kQdzC_LiT","authorship_tag":"ABX9TyOq6oviFMqsxEYxXJwgFLIM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wtNYdIDK-tPH"},"source":["!pip install -q parlai\n","!pip install wandb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISSpR2RxBbgW"},"source":["# !pip install git+https://github.com/thunlp/OpenMatch.git\n","!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqwSFHit_YHv"},"source":["# Import the Interactive script\n","from parlai.scripts.interactive import Interactive\n","import os\n","working_dir = \"/content/drive/MyDrive/Dev/ECEN689/ECEN689\"\n","print(working_dir)\n","\n","\n","# call it with particular args\n","Interactive.main(\n","    model_file=os.path.join(working_dir, \"from_pretrained/model\"),\n","    # model_file='zoo:dodecadialogue/empathetic_dialogues_ft/model',\n","    inference=\"beam\", \n","    beam_size=5, beam_min_length=10, beam_block_ngram=3, beam_context_block_ngram=3\n",")\n","# this is used to try my pre-trained transformer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0vTTFNgc-5VR","executionInfo":{"status":"ok","timestamp":1620075287306,"user_tz":300,"elapsed":1081,"user":{"displayName":"Diya Li","photoUrl":"","userId":"02479113133044853042"}}},"source":["import logging\n","import sys\n","import numpy as np\n","from sklearn.metrics import ndcg_score\n","\n","class User:\n","    \"\"\"\n","    this is used to simuated user in response to agent\n","    the user class is essentially a database of conversations\n","    it search for the generated response like greedy method, \n","    \"\"\"\n","    def __init__(self, dataset, cq_reward, cq_penalty, tolerance = 2, patience = 5):\n","        self.dataset = dataset\n","        self.tolerance = tolerance\n","        self.patience = patience\n","        self.anger = 0\n","        self.cq_reward = cq_reward\n","        self.cq_penalty = cq_penalty\n","\n","    def respond_to_question(self, conversation_id, question):\n","        \"\"\"\n","        responds to a sequence within the context of a give nconverstation\n","        returns 0 if the question is not in the converstation\n","        return 1 if the converstation in given converstaiton \n","        \"\"\"\n","        is_off_topic = True\n","        question_pos = 10000\n","        for pos, utterance in enumerate(self.dataset[conversation_id]):\n","            if question == utterance:\n","                is_off_topic = False\n","                question_pos = pos\n","        \n","        if is_off_topic:\n","            return 0\n","        else:\n","            try:\n","                return self.dataset[conversation_id][question_pos + 1]\n","            except:\n","                #logging.info('The question is the last utterance in the conversation.')\n","                return 1\n","\n","    def initialize_state(self, conversation_id):\n","        '''\n","        Initialize the user state given the conversation id.\n","        '''            \n","        initial_query = self.dataset[conversation_id][0]\n","        try:\n","            initial_query = initial_query[:-1] if initial_query[-1] == '\\n' else initial_query\n","        except:\n","            return ''\n","        self.anger = 0\n","        return initial_query\n","    \n","    def update_state(self, conversation_id, context, action, top_n_question, top_n_answer, use_top_k):\n","        '''\n","        Read the agent action and update the user state, compute reward and return them for save.\n","        The agent action should be 0 (retrieve an answer) or 1 (ask clarifying question)\n","        '''\n","        patience_used = 0\n","        if action == 0:\n","            # agent answer the question, evaluate the answer\n","            n = len(top_n_answer)\n","            context_ = context + ' [SEP] ' + top_n_answer[0]\n","            true_rel = [0] * n\n","            for i in range(n):\n","                try:\n","                    true_rel[i] = self.respond_to_question(conversation_id, top_n_answer[i])\n","                except:\n","                    print(\"Error in conversation: \" + conversation_id)\n","            reward = 0\n","            for i, rel in enumerate(true_rel):\n","                try:\n","                    reward += rel/(i+1)\n","                except:\n","                    reward += 0\n","            \n","            if reward > 1:\n","                reward = 1\n","            return context_, reward, True, None, patience_used\n","        elif action == 1:\n","            # agent asks clarifying question, find corresponding answer in the dataset and return\n","            done = False\n","            correct_question_id = -1\n","            user_response_text = ''\n","            for qid in range(len(top_n_question)):\n","                response = self.respond_to_question(conversation_id, top_n_question[qid])\n","                if type(response) == int:\n","                    continue\n","                else:\n","                    if correct_question_id == -1:\n","                        #logging.info(\"Good CQ.\")\n","                        correct_question_id = qid\n","                        user_response_text = response\n","            if 0 <= correct_question_id <= (use_top_k - 1):\n","                reward = self.cq_reward\n","                context_ = context + ' [SEP] ' + top_n_question[correct_question_id] + ' [SEP] ' + user_response_text\n","                patience_used = correct_question_id\n","            else:\n","                # the agent asks a bad question  \n","                reward = self.cq_penalty\n","                done = True\n","                context_ = context + ' [SEP] ' + top_n_question[0] + ' [SEP] ' + 'This question is not relevant.'\n","            return context_, reward, done, top_n_question[correct_question_id], patience_used\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"O1XcFxn4Aw6F","executionInfo":{"status":"ok","timestamp":1620075389110,"user_tz":300,"elapsed":359,"user":{"displayName":"Diya Li","photoUrl":"","userId":"02479113133044853042"}}},"source":["import json\n","import csv\n","import re\n","import glob\n","\n","class ConversationDataset():\n","    '''\n","    The conversation database class. \n","    '''\n","    def __init__(self, path_to_dataset, batch_size, max_size):\n","        self.batches = []\n","        self.max_len = 512\n","        print(\"Reading data from\", path_to_dataset, \"batch size\", batch_size)\n","        all_data_list = glob.glob(path_to_dataset + '*')\n","        all_data_list.sort()\n","        all_data_list = all_data_list[:max_size] # max size\n","        files_in_batch = 0\n","        for data_file in all_data_list:\n","            f = open(data_file)\n","            data = f.readlines()\n","            data = [d.strip() for d in data]\n","            data_id = data_file.split('/')[-1]\n","            if files_in_batch == 0:\n","                self.batches.append({'conversations':{}, 'responses_pool':[], 'answers_pool':[]})\n","            \n","            self.batches[-1]['conversations'][data_id] = data\n","            for ut_num in range(len(data)):\n","                if ut_num % 2 and ut_num != (len(data) - 1) :\n","                    self.batches[-1]['responses_pool'].append(data[ut_num])\n","            self.batches[-1]['answers_pool'].append(data[-1])\n","            files_in_batch += 1\n","            if files_in_batch == batch_size:\n","                files_in_batch = 0"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Vz0YzgjBJ8g","executionInfo":{"status":"ok","timestamp":1620075573231,"user_tz":300,"elapsed":2254,"user":{"displayName":"Diya Li","photoUrl":"","userId":"02479113133044853042"}}},"source":["import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch as T\n","import transformers\n","import warnings \n","import random\n","import time\n","warnings.filterwarnings(\"ignore\")\n","\n","class LinearDeepQNetwork(nn.Module):\n","    '''\n","    The linear deep Q network used by the agent.\n","    '''\n","    def __init__(self, lr, lr_decay, weight_decay, n_actions, input_dims, hidden_size = 16):\n","        super(LinearDeepQNetwork, self).__init__()\n","        self.fc1 = nn.Linear(input_dims, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, n_actions)\n","\n","        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay = weight_decay)\n","        self.loss = nn.MSELoss()\n","        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","\n","    def forward(self, state):\n","        hidden = F.relu(self.fc1(state))\n","        score = self.fc2(hidden)\n","\n","        return score\n","\n","class LinearDeepNetwork(nn.Module):\n","    '''\n","    The linear deep network used by the agent.\n","    '''\n","    def __init__(self, lr, lr_decay, weight_decay, n_actions, input_dims, hidden_size = 16):\n","        super(LinearDeepNetwork, self).__init__()\n","        self.fc1 = nn.Linear(input_dims, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, n_actions)\n","\n","        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay = weight_decay)\n","        self.loss = nn.MSELoss()\n","        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","\n","    def forward(self, state):\n","        hidden = F.relu(self.fc1(state))\n","        score = F.softmax(self.fc2(hidden))\n","\n","        return score\n","\n","\n","class Agent():\n","    '''\n","    The conversational QA agent.\n","    '''\n","    def __init__(self, input_dims, n_actions, lr, gamma=0.25, lr_decay = 1e-10, weight_decay = 1e-3,\n","                 epsilon=1.0, eps_dec=1e-3, eps_min=0.01, top_k = 1, data_augment = 10):\n","        self.lr = lr\n","        self.lr_decay = lr_decay\n","        self.input_dims = input_dims\n","        self.n_actions = n_actions\n","        self.gamma = gamma\n","        self.weight_decay = weight_decay \n","        self.epsilon = epsilon\n","        self.eps_dec = eps_dec\n","        self.eps_min = eps_min\n","        self.top_k = top_k\n","        self.data_augment = data_augment\n","        self.action_space = [i for i in range(self.n_actions)]\n","        self.experiences = []\n","        self.experiences_replay_times = 3\n","        self.loss_history = []\n","\n","        self.Q = LinearDeepQNetwork(self.lr, self.lr_decay, self.weight_decay, self.n_actions, self.input_dims)\n","        self.device = T.device(\"cuda\")\n","        self.Q.to(self.device)\n","\n","    def choose_action(self, query_embedding, context_embedding, questions_embeddings, answers_embeddings, question_scores, answer_scores):\n","        encoded_q = questions_embeddings[0]\n","        for i in range(1, self.top_k):\n","            encoded_q = T.cat((encoded_q, questions_embeddings[i]), dim=0)\n","            \n","        encoded_state = T.cat((query_embedding, context_embedding), dim=0)\n","        encoded_state = T.cat((encoded_state, encoded_q), dim=0)\n","        encoded_state = T.cat((encoded_state, answers_embeddings[0]), dim=0)\n","        encoded_state = T.cat((encoded_state, question_scores[:self.top_k]), dim=0)\n","        encoded_state = T.cat((encoded_state, answer_scores[:1]), dim=0)\n","    \n","        if np.random.random() > self.epsilon:\n","            state = T.tensor(encoded_state, dtype=T.float).to(self.device)\n","            actions = self.Q.forward(state)\n","            action = T.argmax(actions).item()\n","        else:\n","            action = np.random.choice(self.action_space)\n","        return action\n","\n","    def decrement_epsilon(self):\n","        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n","\n","\n","    def joint_learn(self, state, a_reward, q_reward, state_):\n","        # save to experiences for experience replay\n","        \n","        self.experiences.append([state, a_reward, q_reward, state_])\n","        \n","        if a_reward < q_reward:\n","            for da in range(self.data_augment):\n","                self.experiences.append([state, a_reward, q_reward, state_])\n","        \n","        # sample from past experiences\n","        exps = random.sample(self.experiences, min(self.experiences_replay_times, len(self.experiences)))\n","        exps.append([state, a_reward, q_reward, state_])\n","\n","        for exp in exps:\n","            state, a_reward, q_reward, state_ = exp[0], exp[1], exp[2], exp[3]\n","\n","            query_embedding, context_embedding, questions_embeddings, answers_embeddings, question_scores, answer_scores = state[0], state[1], state[2], state[3], state[4], state[5]\n","            query_embedding, context_embedding_, questions_embeddings_, answers_embeddings_, question_scores_, answer_scores_ = state_[0], state_[1], state_[2], state_[3], state_[4], state_[5]\n","\n","            encoded_q = questions_embeddings[0]\n","            for i in range(1, self.top_k):\n","                encoded_q = T.cat((encoded_q, questions_embeddings[i]), dim=0)\n","\n","            encoded_state = T.cat((query_embedding, context_embedding), dim=0)\n","            encoded_state = T.cat((encoded_state, encoded_q), dim=0)\n","            encoded_state = T.cat((encoded_state, answers_embeddings[0]), dim=0)\n","            encoded_state = T.cat((encoded_state, question_scores[:self.top_k]), dim=0)\n","            encoded_state = T.cat((encoded_state, answer_scores[:1]), dim=0) \n","\n","            encoded_state_ = None\n","            if questions_embeddings_ is not None and answers_embeddings_ is not None:\n","                encoded_q_ = questions_embeddings_[0]\n","                for i in range(1, self.top_k):\n","                    encoded_q_ = T.cat((encoded_q_, questions_embeddings_[i]), dim=0)\n","                \n","                encoded_state_ = T.cat((query_embedding, context_embedding_), dim=0)\n","                encoded_state_ = T.cat((encoded_state_, encoded_q_), dim=0)\n","                encoded_state_ = T.cat((encoded_state_, answers_embeddings_[0]), dim=0)\n","                encoded_state_ = T.cat((encoded_state_, question_scores_[:self.top_k]), dim=0)\n","                encoded_state_ = T.cat((encoded_state_, answer_scores_[:1]), dim=0)\n","            \n","            self.Q.optimizer.zero_grad()\n","            states = T.tensor(encoded_state, dtype=T.float).to(self.device)\n","            a_rewards = T.tensor(a_reward).to(self.device)\n","            q_rewards = T.tensor(q_reward).to(self.device)\n","            states_ = T.tensor(encoded_state_, dtype=T.float).to(self.device) if encoded_state_ is not None else None\n","\n","            pred = self.Q.forward(states)\n","            q_next = self.Q.forward(states_).max() if encoded_state_ is not None else T.tensor(0).to(self.device)\n","            q_target = T.tensor([a_rewards, q_rewards + self.gamma*q_next]).to(self.device) if encoded_state_ is not None else T.tensor([a_rewards, q_rewards]).to(self.device)\n","\n","            loss = self.Q.loss(q_target, pred).to(self.device)\n","            # l1 penalty\n","            l1 = 0\n","            for p in self.Q.parameters():\n","                l1 += p.abs().sum()\n","            \n","            loss = loss + self.weight_decay * l1\n","            self.loss_history.append(loss.item())\n","            loss.backward()\n","            self.Q.optimizer.step()     \n","            \n","        self.decrement_epsilon()\n","\n","\n","class BaseAgent():\n","    '''\n","    The Baseline conversational QA agent.\n","    '''\n","    def __init__(self, input_dims, n_actions, lr, lr_decay = 1e-10, weight_decay = 1e-3):\n","        self.lr = lr\n","        self.lr_decay = lr_decay\n","        self.input_dims = input_dims\n","        self.n_actions = n_actions\n","        self.weight_decay = weight_decay \n","        self.loss_history = []\n","\n","        self.Q = LinearDeepNetwork(self.lr, self.lr_decay, self.weight_decay, self.n_actions, self.input_dims)\n","        self.device = T.device(\"cuda\")\n","        self.Q.to(self.device)\n","\n","    def choose_action(self, query_embedding, context_embedding):\n","        \n","        encoded_state = T.cat((query_embedding, context_embedding), dim=0)\n","        state = T.tensor(encoded_state, dtype=T.float).to(self.device)\n","        actions = self.Q.forward(state)\n","        action = T.argmax(actions).item()\n","        \n","        return action\n","\n","    def learn(self, query_embedding, context_embedding, true_label):\n","        # save to experiences for experience replay     \n","        encoded_state = T.cat((query_embedding, context_embedding), dim=0)\n","       \n","        self.Q.optimizer.zero_grad()\n","        states = T.tensor(encoded_state, dtype=T.float).to(self.device)\n","        \n","        pred = self.Q.forward(states)\n","        q_target = T.tensor([1, 0]).to(self.device) if true_label == 0 else T.tensor([0, 1]).to(self.device)\n","        loss = self.Q.loss(q_target, pred).to(self.device)\n","            \n","        # l1 penalty\n","        l1 = 0\n","        for p in self.Q.parameters():\n","            l1 += p.abs().sum()\n","            \n","        loss = loss + self.weight_decay * l1\n","            \n","        self.loss_history.append(loss.item())\n","\n","        loss.backward()\n","        self.Q.optimizer.step()     \n","            \n","\n","\n","class ScoreAgent():\n","    '''\n","    using only the ranking scores.\n","    '''\n","    def __init__(self, input_dims, n_actions, lr, gamma=0.25, lr_decay = 1e-10, weight_decay = 1e-3,\n","                 epsilon=1.0, eps_dec=1e-3, eps_min=0.01, top_k = 1, data_augment = 10):\n","        self.lr = lr\n","        self.lr_decay = lr_decay\n","        self.input_dims = input_dims\n","        self.n_actions = n_actions\n","        self.gamma = gamma\n","        self.weight_decay = weight_decay \n","        self.epsilon = epsilon\n","        self.eps_dec = eps_dec\n","        self.eps_min = eps_min\n","        self.top_k = top_k\n","        self.data_augment = data_augment\n","        self.action_space = [i for i in range(self.n_actions)]\n","        self.experiences = []\n","        self.experiences_replay_times = 3\n","        self.loss_history = []\n","\n","        self.Q = LinearDeepQNetwork(self.lr, self.lr_decay, self.weight_decay, self.n_actions, self.input_dims)\n","        self.device = T.device(\"cuda\")\n","        self.Q.to(self.device)\n","\n","    def choose_action(self, question_scores, answer_scores):\n","        question_scores = T.tensor(question_scores)\n","        answer_scores = T.tensor(answer_scores)\n","        encoded_state = T.cat((question_scores[:self.top_k], answer_scores[:1]), dim=0)\n","\n","        if np.random.random() > self.epsilon:\n","            'if the random number is greater than exploration threshold, choose the action maximizing Q'\n","            state = T.tensor(encoded_state, dtype=T.float).to(self.device)\n","            actions = self.Q.forward(state)\n","            #print(actions)\n","            action = T.argmax(actions).item()\n","        else:\n","            'randomly choosing an action'\n","            action = np.random.choice(self.action_space)\n","        return action\n","\n","    def decrement_epsilon(self):\n","        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n","\n","\n","    def joint_learn(self, state, a_reward, q_reward, state_):\n","        # save to experiences for experience replay\n","        \n","        self.experiences.append([state, a_reward, q_reward, state_])\n","        \n","        if a_reward < q_reward:\n","            for da in range(self.data_augment):\n","                self.experiences.append([state, a_reward, q_reward, state_])\n","        \n","        # sample from past experiences\n","        exps = random.sample(self.experiences, min(self.experiences_replay_times, len(self.experiences)))\n","        exps.append([state, a_reward, q_reward, state_])\n","\n","        for exp in exps:\n","            state, a_reward, q_reward, state_ = exp[0], exp[1], exp[2], exp[3]\n","\n","            question_scores, answer_scores = state[0], state[1]\n","            question_scores_, answer_scores_ = state_[0], state_[1]\n","\n","            encoded_state = T.cat((question_scores[:self.top_k], answer_scores[:1]), dim=0)\n","            if question_scores_ is not None and answer_scores_ is not None:\n","                encoded_state_ = T.cat((question_scores_[:self.top_k], answer_scores_[:1]), dim=0)\n","            else:\n","                encoded_state_ = None\n","\n","            self.Q.optimizer.zero_grad()\n","            states = T.tensor(encoded_state, dtype=T.float).to(self.device)\n","            a_rewards = T.tensor(a_reward).to(self.device)\n","            q_rewards = T.tensor(q_reward).to(self.device)\n","            states_ = T.tensor(encoded_state_, dtype=T.float).to(self.device) if encoded_state_ is not None else None\n","\n","            pred = self.Q.forward(states)\n","            q_next = self.Q.forward(states_).max() if encoded_state_ is not None else T.tensor(0).to(self.device)\n","            q_target = T.tensor([a_rewards, q_rewards + self.gamma*q_next]).to(self.device) if encoded_state_ is not None else T.tensor([a_rewards, q_rewards]).to(self.device)\n","\n","            loss = self.Q.loss(q_target, pred).to(self.device) \n","            # l1 penalty\n","            l1 = 0\n","            for p in self.Q.parameters():\n","                l1 += p.abs().sum()\n","            \n","            loss = loss + self.weight_decay * l1\n","            self.loss_history.append(loss.item())\n","            loss.backward()\n","            self.Q.optimizer.step()     \n","        self.decrement_epsilon()\n","\n","\n","\n","class TextAgent():\n","    '''\n","    Using only the encoded text.\n","    '''\n","    def __init__(self, input_dims, n_actions, lr, gamma=0.25, lr_decay = 1e-10, weight_decay = 1e-3,\n","                 epsilon=1.0, eps_dec=1e-3, eps_min=0.01, top_k = 1, data_augment = 10):\n","        self.lr = lr\n","        self.lr_decay = lr_decay\n","        self.input_dims = input_dims\n","        self.n_actions = n_actions\n","        self.gamma = gamma\n","        self.weight_decay = weight_decay \n","        self.epsilon = epsilon\n","        self.eps_dec = eps_dec\n","        self.eps_min = eps_min\n","        self.top_k = top_k\n","        self.data_augment = data_augment\n","        self.action_space = [i for i in range(self.n_actions)]\n","        self.experiences = []\n","        self.experiences_replay_times = 3\n","        self.loss_history = []\n","\n","        self.Q = LinearDeepQNetwork(self.lr, self.lr_decay, self.weight_decay, self.n_actions, self.input_dims)\n","        self.device = T.device(\"cuda\")\n","        self.Q.to(self.device)\n","\n","    def choose_action(self, query_embedding, context_embedding, questions_embeddings, answers_embeddings):\n","        # Encode text\n","        \n","        encoded_q = questions_embeddings[0]\n","        for i in range(1, self.top_k):\n","            encoded_q = T.cat((encoded_q, questions_embeddings[i]), dim=0)\n","        encoded_state = T.cat((query_embedding, context_embedding), dim=0)\n","        encoded_state = T.cat((encoded_state, encoded_q), dim=0)\n","        encoded_state = T.cat((encoded_state, answers_embeddings[0]), dim=0)\n","        \n","        if np.random.random() > self.epsilon:\n","            'if the random number is greater than exploration threshold, choose the action maximizing Q'\n","            state = T.tensor(encoded_state, dtype=T.float).to(self.device)\n","            actions = self.Q.forward(state)\n","            #print(actions)\n","            action = T.argmax(actions).item()\n","        else:\n","            'randomly choosing an action'\n","            action = np.random.choice(self.action_space)\n","        return action\n","\n","    def decrement_epsilon(self):\n","        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n","\n","\n","    def joint_learn(self, state, a_reward, q_reward, state_):\n","        # save to experiences for experience replay\n","        \n","        self.experiences.append([state, a_reward, q_reward, state_])\n","        \n","        if a_reward < q_reward:\n","            for da in range(self.data_augment):\n","                self.experiences.append([state, a_reward, q_reward, state_])\n","        \n","        # sample from past experiences\n","        exps = random.sample(self.experiences, min(self.experiences_replay_times, len(self.experiences)))\n","        exps.append([state, a_reward, q_reward, state_])\n","\n","        for exp in exps:\n","            state, a_reward, q_reward, state_ = exp[0], exp[1], exp[2], exp[3]\n","\n","            query_embedding, context_embedding, questions_embeddings, answers_embeddings= state[0], state[1], state[2], state[3]\n","            query_embedding, context_embedding_, questions_embeddings_, answers_embeddings_ = state_[0], state_[1], state_[2], state_[3]\n","\n","            encoded_q = questions_embeddings[0]\n","            for i in range(1, self.top_k):\n","                encoded_q = T.cat((encoded_q, questions_embeddings[i]), dim=0)\n","\n","            encoded_state = T.cat((query_embedding, context_embedding), dim=0)\n","            encoded_state = T.cat((encoded_state, encoded_q), dim=0)\n","            encoded_state = T.cat((encoded_state, answers_embeddings[0]), dim=0)\n","\n","            encoded_state_ = None\n","            if questions_embeddings_ is not None and answers_embeddings_ is not None:\n","\n","                encoded_q_ = questions_embeddings_[0]\n","                for i in range(1, self.top_k):\n","                    encoded_q_ = T.cat((encoded_q_, questions_embeddings_[i]), dim=0)\n","                \n","                encoded_state_ = T.cat((query_embedding, context_embedding_), dim=0)\n","                encoded_state_ = T.cat((encoded_state_, encoded_q_), dim=0)\n","                encoded_state_ = T.cat((encoded_state_, answers_embeddings_[0]), dim=0)\n","            \n","            self.Q.optimizer.zero_grad()\n","            states = T.tensor(encoded_state, dtype=T.float).to(self.device)\n","            a_rewards = T.tensor(a_reward).to(self.device)\n","            q_rewards = T.tensor(q_reward).to(self.device)\n","            states_ = T.tensor(encoded_state_, dtype=T.float).to(self.device) if encoded_state_ is not None else None\n","\n","            pred = self.Q.forward(states)\n","            q_next = self.Q.forward(states_).max() if encoded_state_ is not None else T.tensor(0).to(self.device)\n","            q_target = T.tensor([a_rewards, q_rewards + self.gamma*q_next]).to(self.device) if encoded_state_ is not None else T.tensor([a_rewards, q_rewards]).to(self.device)\n","\n","            loss = self.Q.loss(q_target, pred).to(self.device)\n","            # l1 penalty\n","            l1 = 0\n","            for p in self.Q.parameters():\n","                l1 += p.abs().sum()\n","            \n","            loss = loss + self.weight_decay * l1\n","            self.loss_history.append(loss.item())\n","            loss.backward()\n","            self.Q.optimizer.step()     \n","        self.decrement_epsilon()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFjfQltxFbMr","executionInfo":{"status":"ok","timestamp":1620078196593,"user_tz":300,"elapsed":715,"user":{"displayName":"Diya Li","photoUrl":"","userId":"02479113133044853042"}}},"source":["cp /content/drive/MyDrive/Dev/ECEN689/ECEN689/interactive.py ./"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"ax4TfKnHBTBg","executionInfo":{"status":"ok","timestamp":1620078201342,"user_tz":300,"elapsed":345,"user":{"displayName":"Diya Li","photoUrl":"","userId":"02479113133044853042"}}},"source":["import logging\n","import numpy as np\n","import random\n","import json\n","import resource\n","import csv\n","from transformers import AutoTokenizer, AutoModel\n","from scipy.special import softmax\n","import sys\n","import time\n","from interactive import Interactive, rerank\n","from copy import deepcopy\n","import argparse\n","import psutil\n","\n","\n","observation_dim = 768\n","action_num = 2\n","cq_reward = 0.11\n","cq_penalty = cq_reward - 1\n","agent_gamma = -cq_penalty\n","train_iter = 50\n","batch_size = 100\n","max_round = 5 # max conversation round\n","max_train_size = 10000\n","max_test_size = int(0.25*max_train_size)\n","\n","def limit_memory(maxsize): \n","    soft, hard = resource.getrlimit(resource.RLIMIT_AS) \n","    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard)) \n","\n","def generate_embedding_no_grad(text, tokenizer, embedding_model):\n","    with T.no_grad():\n","        tokenized_context_ = T.tensor([tokenizer.encode(text, add_special_tokens=True)])\n","        context_embedding_ = T.squeeze(embedding_model(tokenized_context_)[0])[0] \n","        del tokenized_context_\n","        return context_embedding_\n","\n","def read_from_memory(query, context, memory):\n","    return memory[query]['embedding'], memory[query][context]['embedding'],\\\n","        memory[query][context]['questions'], memory[query][context]['answers'],\\\n","        memory[query][context]['questions_embeddings'],memory[query][context]['answers_embeddings'],\\\n","        memory[query][context]['questions_scores'], memory[query][context]['answers_scores']\n","\n","def save_to_memory(query, context, memory, questions, answers, questions_scores, answers_scores, tokenizer, embedding_model):\n","    if query not in memory.keys():\n","        memory[query] = {}\n","        with T.no_grad():\n","            tokenized_query = T.tensor([tokenizer.encode(query, add_special_tokens=True)])\n","            memory[query]['embedding'] = T.squeeze(embedding_model(tokenized_query)[0])[0]\n","    \n","    memory[query][context] = {}\n","    with T.no_grad():\n","        memory[query][context]['embedding'] = T.squeeze(embedding_model(T.tensor([tokenizer.encode(context, add_special_tokens=True)]))[0])[0]\n","        memory[query][context]['questions_embeddings'] = [T.squeeze(embedding_model(T.tensor([tokenizer.encode(questions[i], add_special_tokens=True)]))[0])[0] for i in range(3)]\n","        memory[query][context]['answers_embeddings'] = [T.squeeze(embedding_model(T.tensor([tokenizer.encode(answers[0], add_special_tokens=True)]))[0])[0]]\n","    memory[query][context]['questions'] = questions\n","    memory[query][context]['answers'] = answers\n","    memory[query][context]['questions_scores'] = T.tensor(questions_scores)\n","    memory[query][context]['answers_scores'] = T.tensor(answers_scores)\n","    return memory\n","\n","def generate_batch_question_candidates(batch, conversation_id, ignore_questions, total_candidates):\n","    positives = [batch['conversations'][conversation_id][turn_id] for turn_id in range(len(batch['conversations'][conversation_id])) if turn_id % 2 == 1 and turn_id != len(batch['conversations'][conversation_id])-1]\n","    filtered_positives = [cand for cand in positives if cand not in ignore_questions]\n","    negatives = [response for response in batch['responses_pool'] if response not in positives][:total_candidates - len(filtered_positives)]\n","    return filtered_positives + negatives\n","\n","def generate_batch_answer_candidates(batch, conversation_id, total_candidates):\n","    positives = [batch['conversations'][conversation_id][-1]]\n","    negatives = [answer for answer in batch['answers_pool'] if answer not in positives][:total_candidates - len(positives)] \n","    return positives + negatives\n","\n","def main(args):\n","    logging.getLogger().setLevel(logging.INFO)\n","    limit_memory(1e11)\n","\n","    random.seed(2020)\n","    somestuff = None\n","    if args.cv != -1:\n","        train_dataset = somestuff\n","        test_dataset = somestuff\n","    else:\n","        train_dataset = somestuff\n","        test_dataset = seomstuff\n","    agent = Agent(lr = 1e-4, \n","                  input_dims = (3 + args.topn) * observation_dim + 1 + args.topn, \n","                  top_k = args.topn, \n","                  n_actions=action_num, gamma = agent_gamma, weight_decay = 0.01)\n","    score_agent = ScoreAgent(lr = 1e-4, \n","                             input_dims = 1 + args.topn, \n","                             top_k = args.topn, \n","                             n_actions=action_num, \n","                             gamma = agent_gamma, \n","                             weight_decay = 0.0)\n","    text_agent = TextAgent(lr = 1e-4, \n","                           input_dims = (3 + args.topn) * observation_dim,\n","                           top_k = args.topn, n_actions=action_num, \n","                           gamma = agent_gamma, \n","                           weight_decay = 0.01)\n","    base_agent = BaseAgent(lr = 1e-4, \n","                           input_dims = 2 * observation_dim, \n","                           n_actions = 2, weight_decay = 0.01)\n","    question_reranker = Interactive.main(\n","        model_file=os.path.join(working_dir, \"from_pretrained/model\"),\n","    )\n","    \n","\n"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwwsLpZqCVds","executionInfo":{"status":"ok","timestamp":1620080375558,"user_tz":300,"elapsed":19334,"user":{"displayName":"Diya Li","photoUrl":"","userId":"02479113133044853042"}},"outputId":"eedcb401-7d27-4d84-b6c0-2f8a6692f887"},"source":["# print(Interactive.help(model_file=os.path.join(working_dir, \"from_pretrained/model\")))\n","Interactive.main(\n","    model_file=os.path.join(working_dir, \"from_pretrained/model\")\n",")"],"execution_count":34,"outputs":[{"output_type":"stream","text":["22:19:16 | \u001b[33mOverriding opt[\"model_file\"] to /content/drive/MyDrive/Dev/ECEN689/ECEN689/from_pretrained/model (previously: from_pretrained/model)\u001b[0m\n","22:19:16 | Using CUDA\n","22:19:16 | loading dictionary from /content/drive/MyDrive/Dev/ECEN689/ECEN689/from_pretrained/model.dict\n","22:19:16 | num words = 54944\n","22:19:16 | TransformerGenerator: full interactive mode on.\n","22:19:18 | Total parameters: 87,508,992 (87,508,992 trainable)\n","22:19:18 | Loading existing model params from /content/drive/MyDrive/Dev/ECEN689/ECEN689/from_pretrained/model\n","\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n","22:19:35 | creating task(s): interactive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<parlai.tasks.interactive.worlds.InteractiveWorld at 0x7f9cb716ac90>"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"ZfYnHbi6Tw5c"},"source":[""],"execution_count":null,"outputs":[]}]}